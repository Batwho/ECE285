{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28216,"status":"ok","timestamp":1685478952614,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"PLzwChbevmfk","outputId":"64ef3453-513f-4217-d302-249577649a01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting decord\n","  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.22.4)\n","Installing collected packages: decord\n","Successfully installed decord-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting icecream\n","  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n","Collecting colorama>=0.3.9 (from icecream)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.14.0)\n","Collecting executing>=0.3.1 (from icecream)\n","  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n","Collecting asttokens>=2.0.1 (from icecream)\n","  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n","Installing collected packages: executing, colorama, asttokens, icecream\n","Successfully installed asttokens-2.2.1 colorama-0.4.6 executing-1.2.0 icecream-2.1.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rarfile\n","  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n","Installing collected packages: rarfile\n","Successfully installed rarfile-4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unrar\n","  Downloading unrar-0.4-py3-none-any.whl (25 kB)\n","Installing collected packages: unrar\n","Successfully installed unrar-0.4\n"]}],"source":["# Run it once (in one session)\n","!pip install decord\n","!pip install einops\n","!pip install icecream\n","!pip install rarfile\n","!pip install unrar"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7995,"status":"ok","timestamp":1685479062270,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"0YxhuZ85swSE"},"outputs":[],"source":["# Imports\n","import torch\n","from torch import nn, einsum\n","from torch.nn import functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision as tv\n","from torch.utils.data import random_split, DataLoader,Dataset\n","import time\n","import random\n","import math\n","import decord\n","import numpy as np\n","import gc\n","from einops import rearrange, repeat,reduce\n","from einops.layers.torch import Rearrange\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from icecream import ic\n","from torchvision.datasets import DatasetFolder\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader, random_split\n","import tensorflow as tf\n","import os\n","import rarfile\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader, random_split\n","import torch.utils.data as data\n","from torchvision import transforms\n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import shutil\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1685479062271,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"NCvAjgaBtRCS","outputId":"c7437b06-a633-407f-e693-7faec7decc0a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["# set device\n","device ='cuda:0' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","source":["# DOWNLOADING DATA FROM SOURCE WEBSITE"],"metadata":{"id":"_3Nv_NnzbWKU"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21471,"status":"ok","timestamp":1685478974078,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"k6MgONYtI5Om","outputId":"3dacf360-9988-4be7-9574-3a1789f65d2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-05-30 20:35:53--  http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n","Resolving serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)... 128.148.254.114\n","Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar [following]\n","--2023-05-30 20:35:53--  https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n","Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2124008126 (2.0G)\n","Saving to: ‘hmdb51_org.rar’\n","\n","hmdb51_org.rar      100%[===================>]   1.98G  84.2MB/s    in 21s     \n","\n","2023-05-30 20:36:14 (96.5 MB/s) - ‘hmdb51_org.rar’ saved [2124008126/2124008126]\n","\n"]}],"source":["# once per session/runtime\n","!wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar "]},{"cell_type":"markdown","source":["# DATA DIRECTORY SETTINGS"],"metadata":{"id":"4LVd8ijibahB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Es-sEYkeJCvB"},"outputs":[],"source":["# once per session/runtime\n","\n","rar_path = '/content/hmdb51_org.rar'  \n","extract_path = '/content/dataset' \n","\n","with rarfile.RarFile(rar_path, 'r') as rar:\n","    rar.extractall(extract_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhalxfoYLmEo"},"outputs":[],"source":["# once per session/runtime\n","\n","direcs = os.listdir(extract_path)\n","\n","for i in direcs:\n","  with rarfile.RarFile(f\"dataset/{i}\", 'r') as rar:\n","    rar.extractall(f\"data/{i.split('.')[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32FfM0Y8UHgP"},"outputs":[],"source":["# once per session/runtime\n","\n","!mkdir data_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsWUn_unSF3u"},"outputs":[],"source":["# once per session/runtime\n","\n","# Define the path to the dataset\n","data_path = '/content/data'\n","data_set = '/content/data_set'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685377410707,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"X08fhEF4YrOs","outputId":"c65c46bc-caa2-428c-a195-6a8873689672"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset modification complete.\n"]}],"source":["data_path = '/content/data'  # Path to the original dataset\n","dataset_dir = '/content/data_set'  # Destination directory for the modified dataset\n","\n","# Iterate over the class folders in the original dataset\n","for class_folder in os.listdir(data_path):\n","    class_folder_path = os.path.join(data_path, class_folder)\n","\n","    # Iterate over the subfolders within each class folder\n","    for subfolder in os.listdir(class_folder_path):\n","        subfolder_path = os.path.join(class_folder_path, subfolder)\n","\n","        # Iterate over the files within each subfolder\n","        for file_name in os.listdir(subfolder_path):\n","            file_path = os.path.join(subfolder_path, file_name)\n","\n","            # Create the destination directory in the modified dataset\n","            dest_dir = os.path.join(dataset_dir, subfolder)\n","            os.makedirs(dest_dir, exist_ok=True)\n","\n","            # Move the file to the destination directory\n","            shutil.move(file_path, dest_dir)\n","\n","print(\"Dataset modification complete.\") # success message"]},{"cell_type":"markdown","source":["# DATA LOADING"],"metadata":{"id":"xEu8eeCWaa1k"}},{"cell_type":"code","source":["# Dataset Class\n","class HMDB51Dataset(data.Dataset):\n","    def __init__(self, dataset_dir, frames_per_clip=16):\n","        super().__init__()\n","        self.dataset_dir = dataset_dir\n","        self.frames_per_clip = frames_per_clip\n","        self.video_list = []\n","        self.labels = []\n","\n","        # Get the list of video directories\n","        video_dirs = sorted(os.listdir(dataset_dir))\n","        \n","        for label, video_dir in enumerate(video_dirs):\n","            video_files = os.listdir(os.path.join(dataset_dir, video_dir))\n","            self.video_list.extend([os.path.join(video_dir, video_file) for video_file in video_files])\n","            self.labels.extend([label] * len(video_files))\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, idx):\n","        video_path = os.path.join(self.dataset_dir, self.video_list[idx])\n","        vid = decord.VideoReader(video_path, ctx=decord.cpu(0))\n","        nframes = len(vid)\n","\n","        # If the number of frames in the video is less than frames_per_clip, repeat the frames\n","        if nframes <= self.frames_per_clip:\n","            frame_idxs = torch.arange(0, self.frames_per_clip) % nframes\n","        # Else, sample uniformly separated frames\n","        else:\n","            frame_idxs = torch.linspace(0, nframes - 1, self.frames_per_clip).long()\n","\n","        frames = []\n","        for frame_idx in frame_idxs:\n","            frame_idx = frame_idx.item()  # Convert to scalar value\n","            frame = Image.fromarray(vid[frame_idx].asnumpy())\n","            frame = self.transform(frame)\n","            frames.append(frame)\n","\n","        frames = torch.stack(frames)\n","\n","        label = self.labels[idx]\n","        return frames, label"],"metadata":{"id":"JYSQszRXU8Rw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Directory of the HMDB51 dataset\n","dataset_dir = \"/content/data_set\"\n","\n","# Instantiate the dataset\n","hmdb51_dataset = HMDB51Dataset(dataset_dir)"],"metadata":{"id":"AwQ3zMsXU_kn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TRAIN TEST SPLIT"],"metadata":{"id":"JJd7t1xHal8I"}},{"cell_type":"code","source":["# Split the dataset into train, validation, and test sets\n","train_len = int(0.85 * len(hmdb51_dataset))\n","train_data, val_data = torch.utils.data.random_split(hmdb51_dataset, [train_len, len(hmdb51_dataset) - train_len])\n","\n","# Data loading parameters\n","batch_size = 64\n","test_batch_size = 1\n","num_workers = 0\n","pin_memory = True\n","num_classes = len(set(hmdb51_dataset.labels))\n","\n","frames_per_clip = 16"],"metadata":{"id":"kMkhM0kzVBkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataloaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n","test_loader = torch.utils.data.DataLoader(hmdb51_dataset, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)"],"metadata":{"id":"TOgb3MLjVDb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate and create train-val-test split\n","train_val_data = HMDB51Dataset(dataset_dir, frames_per_clip=frames_per_clip)\n","train_len = int(0.85 * len(train_val_data))\n","train_val_split = [train_len, len(train_val_data) - train_len]\n","train_data, val_data = random_split(train_val_data, train_val_split)\n","test_data = HMDB51Dataset(dataset_dir, frames_per_clip=frames_per_clip)\n","\n","# Print the number of samples in each split\n","print(f\"Train samples: {len(train_data)}\")\n","print(f\"Validation samples: {len(val_data)}\")\n","print(f\"Test samples: {len(test_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uRDrPMCVDyL","executionInfo":{"status":"ok","timestamp":1685378429669,"user_tz":-300,"elapsed":602,"user":{"displayName":"Math Fever","userId":"04770446737635186984"}},"outputId":"411f1678-775c-41cc-dc26-24ad0c84d588"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train samples: 5751\n","Validation samples: 1015\n","Test samples: 6766\n"]}]},{"cell_type":"markdown","source":["# DEFINING MODEL"],"metadata":{"id":"JgEk-y_vawwB"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    \"\"\"\n","    Builds a simple feed forward network\n","    Args:\n","    - dim: (int) - inner dimension of embeddings\n","    - inner_dim: (int) - dimension of transformer head\n","    - n_class: (int) - number of output classes\n","    - encoder: the DinoVisionTransformer encoder\n","    \"\"\"\n","    def __init__(self, dim, inner_dim, n_class, encoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, n_class)\n","        )\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape\n","        x = x.reshape(B*T, C, H, W)\n","        output = self.encoder(x)\n","        output = output.reshape(B, T, -1)\n","        avg = output.mean(dim=1)  # Average pooling over time\n","        return self.mlp(avg)"],"metadata":{"id":"b4qEV8TIWtPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model\n","dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', force_reload=True)\n","dinov2_vits14.to(device)\n","for param in dinov2_vits14.parameters():\n","    param.requires_grad = False\n","\n","model = MLP(384, 512, 101, dinov2_vits14)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KA97YTSWvSe","executionInfo":{"status":"ok","timestamp":1685378432273,"user_tz":-300,"elapsed":1988,"user":{"displayName":"Math Fever","userId":"04770446737635186984"}},"outputId":"00fdce34-4d66-417c-91c1-1503683f1475"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]},{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (encoder): DinoVisionTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n","      (norm): Identity()\n","    )\n","    (blocks): ModuleList(\n","      (0-11): 12 x NestedTensorBlock(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): MemEffAttention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): LayerScale()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): LayerScale()\n","        (drop_path2): Identity()\n","      )\n","    )\n","    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","    (head): Identity()\n","  )\n","  (mlp): Sequential(\n","    (0): Linear(in_features=384, out_features=101, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["## SPECIFYING HYPER-PARAMETERS"],"metadata":{"id":"3wC4B8W6a3aR"}},{"cell_type":"code","source":["# Define the loss function and optimizer\n","loss_criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.01)"],"metadata":{"id":"eON3ZEAhWy3H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MODEL TRAINING & EVALUATION"],"metadata":{"id":"F27JEfspa7Fd"}},{"cell_type":"code","source":["# Training loop\n","epochs = 5\n","for epoch in tqdm(range(1, epochs + 1)):\n","    model.train()\n","    total_epoch_loss = 0\n","    for batch_id, (video_data, labels) in enumerate(train_loader):\n","        video_data, labels = video_data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        prediction = model(video_data)\n","        loss = loss_criterion(prediction, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_epoch_loss += loss.item()\n","        # Add any additional training metrics/logging you need\n","        \n","    # Perform validation at the end of each epoch\n","    model.eval()\n","    total_loss = 0\n","    corrects = 0\n","    with torch.no_grad():\n","        for batch_id, (video_data, labels) in enumerate(val_loader):\n","            video_data, labels = video_data.to(device), labels.to(device)\n","            prediction = model(video_data)\n","            loss = loss_criterion(prediction, labels)\n","            total_loss += loss.item()\n","            corrects += (torch.argmax(prediction, dim=1) == labels).sum()\n","\n","    accuracy = corrects / (len(val_loader) * batch_size)\n","    print(f\"\\n[Val Epoch]: {epoch} , Accuracy: {accuracy}, Valid Loss: {total_loss / len(val_loader)}\")"],"metadata":{"id":"Fdwgd-zOW3Z-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the trained model\n","torch.save(model, \"model.pth\")"],"metadata":{"id":"3KUkL2HwVWQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UazKt6g8WNMn"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}