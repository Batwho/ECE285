{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "q_b_ifobbjU7",
      "metadata": {
        "id": "q_b_ifobbjU7"
      },
      "source": [
        "#Code for ViViT model [[paper]](https://arxiv.org/abs/2103.15691)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "rkt-0enhb0qG",
      "metadata": {
        "id": "rkt-0enhb0qG"
      },
      "source": [
        "## Imports and Global declarations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "angry-effectiveness",
      "metadata": {
        "id": "angry-effectiveness"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision as tv\n",
        "from torch.utils.data import random_split, DataLoader,Dataset\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "# !pip install einops icecream\n",
        "import decord\n",
        "import numpy as np\n",
        "import gc\n",
        "from einops import rearrange, repeat,reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from icecream import ic\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8ifOyxNa5Cnp",
      "metadata": {
        "id": "8ifOyxNa5Cnp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set device\n",
        "device ='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "C9gk3l2-rSt1",
      "metadata": {
        "id": "C9gk3l2-rSt1"
      },
      "outputs": [],
      "source": [
        "# Instantiate tensorboard writer\n",
        "#tb_writer = SummaryWriter()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "yDjl36ascCbH",
      "metadata": {
        "id": "yDjl36ascCbH"
      },
      "source": [
        "## DataLoader for UCF101 dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "rational-neutral",
      "metadata": {
        "id": "rational-neutral"
      },
      "outputs": [],
      "source": [
        "# dataset params\n",
        "frames_per_clip = 8\n",
        "dataset_dir=\"./ucf/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist\"\n",
        "video_dir = \"./ucf/UCF101/UCF-101\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "documentary-python",
      "metadata": {
        "id": "documentary-python"
      },
      "outputs": [],
      "source": [
        "# Dataset Class\n",
        "class UCFDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset Class for reading UCF101 dataset  \n",
        "    \n",
        "    Args:\n",
        "        dataset_dir: (str) - root directory of dataset\n",
        "        subset: (str) - train or test subset\n",
        "        video_list_file: (str) - file name containing list of video names \n",
        "        frames_per_clip: (int) - number of frames to be read in every video clip [default:16]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_dir, subset, video_list_file, frames_per_clip=16):\n",
        "        super().__init__()\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.video_dir = video_dir\n",
        "        self.subset=subset\n",
        "        self.video_list_file = video_list_file\n",
        "        self.video_list = []\n",
        "        self.labels = []\n",
        "        self.indices = []\n",
        "\n",
        "        for i in [1,2,3]:\n",
        "            with open(f'{dataset_dir}/{video_list_file}{str(i)}.txt') as video_names_file:\n",
        "                if self.subset==\"train\":\n",
        "                    tempvideo_list,templabels = zip(*(files[:-1].split() for files in video_names_file.readlines()))\n",
        "                    self.video_list += tempvideo_list\n",
        "                    self.labels += templabels\n",
        "                else:\n",
        "                    tempvideo_list = [files[:-1] for files in video_names_file.readlines()]\n",
        "                    templabels = [None]\n",
        "                    self.video_list += tempvideo_list\n",
        "                    self.labels += templabels\n",
        "                    # with open(f'{dataset_dir}/classInd.txt') as classIndices:\n",
        "                    #     values,keys=zip(*(files[:-1].split() for files in classIndices.readlines()))\n",
        "                    #     tempindices = dict( (k,v) for k,v in zip(keys,values))\n",
        "                    \n",
        "            \n",
        "            #self.indices.append(tempindices)\n",
        "        \n",
        "\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "\n",
        "        self.transform = tv.transforms.Compose([\n",
        "          #tv.transforms.GaussianBlur(9, sigma=(0.1, 2.0)),\n",
        "          tv.transforms.Resize(256,interpolation=tv.transforms.InterpolationMode.BICUBIC),\n",
        "          tv.transforms.CenterCrop(224),\n",
        "          tv.transforms.ToTensor(),\n",
        "          tv.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        videoname = f'{self.video_list[idx]}'\n",
        "        vid = decord.VideoReader(f'{self.video_dir}/{videoname}', ctx=decord.cpu(0))\n",
        "        nframes = len(vid)\n",
        "\n",
        "        # if number of frames of video is less than frames_per_clip, repeat the frames\n",
        "        if nframes <= self.frames_per_clip:\n",
        "            idxs = np.arange(0, self.frames_per_clip).astype(np.int32)\n",
        "            idxs[nframes:] %= nframes\n",
        "\n",
        "        # else if frames_per_clip is greater, sample uniformly seperated frames\n",
        "        else:\n",
        "            idxs = np.linspace(0, nframes-1, self.frames_per_clip)\n",
        "            idxs = np.round(idxs).astype(np.int32)\n",
        "\n",
        "        imgs = []\n",
        "        for k in idxs:\n",
        "            frame = Image.fromarray(vid[k].asnumpy())\n",
        "            frame = self.transform(frame)\n",
        "            imgs.append(frame)\n",
        "        imgs = torch.stack(imgs)\n",
        "\n",
        "        # if its train subset, return both the frames and the label \n",
        "        if self.subset==\"train\":\n",
        "            label = int(self.labels[idx]) - 1    \n",
        "        # else, for test subset, read the label index\n",
        "        else:\n",
        "            classttl = {}\n",
        "            with open(f'{dataset_dir}/classInd.txt') as clsidx:\n",
        "                \n",
        "                classttl = {v:int(k) for k, v in (l.split() for l in clsidx)}\n",
        "\n",
        "            clsname = videoname.split('/')[0]\n",
        "            label= classttl[clsname] -1\n",
        "        return imgs,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6zXGDy7QCpHy",
      "metadata": {
        "id": "6zXGDy7QCpHy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 24434\n",
            "Validation samples: 4313\n",
            "Test samples: 11213\n"
          ]
        }
      ],
      "source": [
        "#Instantiate and create train-val-test split\n",
        "\n",
        "train_val_data = UCFDataset( dataset_dir = dataset_dir, subset=\"train\", video_list_file=\"trainlist0\",frames_per_clip=frames_per_clip)\n",
        "\n",
        "train_len=int(0.85*len(train_val_data))\n",
        "train_val_split = [ train_len, len(train_val_data) - train_len ] \n",
        "\n",
        "train_data , val_data = random_split(train_val_data,train_val_split)\n",
        "test_data = UCFDataset( dataset_dir = dataset_dir, subset=\"test\", video_list_file=\"testlist0\" ,frames_per_clip=frames_per_clip)\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "rotary-ladder",
      "metadata": {
        "id": "rotary-ladder",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3, 224, 224])\n",
            "17\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#example frame from a random video to check if dataloader's working properly\n",
        "plt.imshow(((test_data[35][0][0].permute(1,2,0)*255).int()).numpy())\n",
        "print(train_data[35][0].shape)  #Size([8, 3, 224, 224])\n",
        "print(train_data[35][1])        # label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "adapted-aviation",
      "metadata": {
        "id": "adapted-aviation"
      },
      "outputs": [],
      "source": [
        "# data loading params\n",
        "batch_size = 64\n",
        "test_batch_size = 1\n",
        "num_workers = 8\n",
        "pin_memory = True\n",
        "num_classes=101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "accepting-payment",
      "metadata": {
        "id": "accepting-payment"
      },
      "outputs": [],
      "source": [
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=test_batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "YqviNYNCLiW4",
      "metadata": {
        "id": "YqviNYNCLiW4"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "comparable-crack",
      "metadata": {
        "id": "comparable-crack"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Builds a simple feed forward network\n",
        "    \n",
        "    Args:\n",
        "        dim: (int) - inner dimension of| embeddings \n",
        "        inner_dim: (int) - dimension of transformer head  \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, inner_dim,n_class,encoder):     #dim would be the output image feature from dinov2\n",
        "                                                        \n",
        "        super().__init__()\n",
        "        # mlp with GELU activation function\n",
        "        self.encoder = encoder\n",
        "        #self.adapter = adapter\n",
        "        # self.preprocess = nn.Sequential(\n",
        "        #     nn.Conv2d(in_channels=3,out_channels=width,kernel_size=patch_size,stride=patch_size,bias=False)\n",
        "            \n",
        "        # )\n",
        "        self.mlp = nn.Sequential(\n",
        "            #nn.Linear(dim, inner_dim),\n",
        "            #nn.GELU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            #nn.Linear(inner_dim, inner_dim),\n",
        "            #nn.GELU(),\n",
        "            nn.Linear(dim, n_class),\n",
        "            #nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is [256,8,3,224,224]\n",
        "        \n",
        "        #print('x shape is',x.shape)\n",
        "        # avg = []\n",
        "        \n",
        "        # for i in range(8):\n",
        "        #     xi = x[:,i,:]\n",
        "            \n",
        "        #     #encode x to [8,384]\n",
        "        #     e = []\n",
        "        #     with torch.no_grad():\n",
        "        #         # features_dict = self.encoder.forward_features(xi)\n",
        "        #         # e = features_dict['x_norm_patchtokens']\n",
        "        #         e = self.encoder(xi).reshape(x.shape[0],1,384)\n",
        "        #     print('e shape ',e.shape)   #256,1,384\n",
        "        #     avg.append(e)\n",
        "        # avg = torch.cat(avg,dim=1)    \n",
        "        \n",
        "               \n",
        "        #avg = reduce(avg, \"f t c -> f c\",'mean')        #[16,384]   (b, l)\n",
        "        B,T,C,H,W = x.shape\n",
        "        #x = self.adapter(x,T)\n",
        "\n",
        "        with torch.no_grad():    \n",
        "            \n",
        "            x = x.reshape(B*T,C,H,W) # b c t h w -> b t c h w -> b*t c h w\n",
        "            #print('x shape is',x.shape)\n",
        "            output = self.encoder(x)\n",
        "            output = output.reshape(B,T,-1) # b*t d -> b t d\n",
        "            avg = output.mean(dim=-2) # b t d -> b d\n",
        "        #output = linear_classifier(output) # b d -> b l\n",
        "        #print(avg.shape)\n",
        "        \n",
        "        return self.mlp(avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "669d206b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Adapter(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, adapter_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_channels, adapter_channels)\n",
        "        self.conv = nn.Conv3d(\n",
        "            adapter_channels, adapter_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=(1, 1, 1),\n",
        "            padding=tuple(x // 2 for x in kernel_size),\n",
        "            groups=adapter_channels,\n",
        "        )\n",
        "        self.fc2 = nn.Linear(adapter_channels, in_channels)\n",
        "       \n",
        "        nn.init.constant_(self.conv.weight, 0.)\n",
        "        nn.init.constant_(self.conv.bias, 0.)\n",
        "        nn.init.constant_(self.fc1.bias, 0.)\n",
        "        nn.init.constant_(self.fc2.bias, 0.)\n",
        "\n",
        "    def forward(self, x, T):\n",
        "        x = rearrange(x,'b t c h w-> (b t) (h w) c',t=T)\n",
        "        BT, L, C = x.size()\n",
        "        #print(x.size())\n",
        "        B = BT // T\n",
        "        Ca = self.conv.in_channels      #adapter_channels\n",
        "        H = W = round(math.sqrt(L))\n",
        "        assert L == H * W       #was L-1, without concat, should be L\n",
        "        x_id = x\n",
        "        #x = x[:, 1:, :]\n",
        "        print('x shape b',x.size())\n",
        "        x = self.fc1(x)\n",
        "        print('x shape',x.size())\n",
        "        x = x.view(B, T, H, W, Ca).permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        cudnn_enabled = torch.backends.cudnn.enabled\n",
        "        torch.backends.cudnn.enabled = cudnn_enabled and True\n",
        "        x = self.conv(x)\n",
        "        torch.backends.cudnn.enabled = cudnn_enabled\n",
        "\n",
        "        x = x.permute(0, 2, 3, 4, 1).contiguous().view(BT, L, Ca)\n",
        "        x = self.fc2(x)\n",
        "        x_id += x\n",
        "        print(x_id.size())\n",
        "        return x_id\n",
        "    \n",
        "class LayerNorm(nn.LayerNorm):\n",
        "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        orig_type = x.dtype\n",
        "        ret = super().forward(x.type(torch.float32))\n",
        "        return ret.type(orig_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "economic-limitation",
      "metadata": {
        "id": "economic-limitation"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/z3qian/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (encoder): DinoVisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0-11): 12 x NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "        (fc1): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (conv): Conv3d(384, 384, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), groups=384)\n",
              "        (fc2): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "    (head): Identity()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=384, out_features=101, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#instantiate model and plot on tensorboard\n",
        "#model = ViViT(image_size=224, patch_size=16, num_classes=num_classes, frames_per_clip=frames_per_clip)\n",
        "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')  #,force_reload=True\n",
        "dinov2_vits14.to(device)\n",
        "for param in dinov2_vits14.parameters():\n",
        "    param.requires_grad= False\n",
        "for i in range(12):\n",
        "    #dinov2_vits14.blocks[i].add_module('ln',nn.LayerNorm(384))\n",
        "    dinov2_vits14.blocks[i].add_module('fc1',nn.Linear(384, 384, bias=True))\n",
        "    dinov2_vits14.blocks[i].add_module('conv',nn.Conv3d(384, 384, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), groups=384))\n",
        "    dinov2_vits14.blocks[i].add_module('fc2',nn.Linear(384, 384, bias=True))\n",
        "#adapter = Adapter(768,384,(3,1,1))\n",
        "#adapter.to(device)\n",
        "model = MLP(384,512,101,dinov2_vits14)\n",
        "#frames, _ = next(iter(train_loader))\n",
        "#tb_writer.add_graph(model, frames)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "407c42e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torchvision.models as models\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "# resnet152 = models.resnet152(pretrained=True)\n",
        "# modules=list(resnet152.children())[:-1]\n",
        "# resnet152=nn.Sequential(*modules)\n",
        "# for p in resnet152.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "# img = torch.Tensor(1,3, 224, 224).normal_() # random image\n",
        "# img_var = Variable(img) # assign it to a variable\n",
        "# features_var = resnet152(img_var) # get the output from the last hidden layer of the pretrained resnet\n",
        "# features = features_var.data \n",
        "\n",
        "# model = MLP(2048,512,101,resnet152)\n",
        "# #frames, _ = next(iter(train_loader))\n",
        "# #tb_writer.add_graph(model, frames)\n",
        "# model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "HiyjllqxL9bV",
      "metadata": {
        "id": "HiyjllqxL9bV"
      },
      "source": [
        "## Training utils "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "rY9aVH4pZM4D",
      "metadata": {
        "id": "rY9aVH4pZM4D"
      },
      "outputs": [],
      "source": [
        "# training hyper-params\n",
        "lr=0.01\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "v3JiODtrd0j-",
      "metadata": {
        "id": "v3JiODtrd0j-"
      },
      "outputs": [],
      "source": [
        "# define the loss and optimizers\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, weight_decay=0,lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1_000_00, eta_min=0)\n",
        "#optimizer = torch.optim.Adam(model.parameters(),lr)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=0.9,weight_decay=0.01)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.95)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "least-visitor",
      "metadata": {
        "id": "least-visitor"
      },
      "outputs": [],
      "source": [
        "# training step for every epoch\n",
        "def train_step(loader,epoch,):\n",
        "    \n",
        "    model.train()\n",
        "    # model.encoder.eval()\n",
        "    total_epoch_loss=0\n",
        "    \n",
        "    for batch_id, (video_data,labels) in enumerate(loader):\n",
        "\n",
        "        # video_data,labels = video_data.to(device), labels.to(device)\n",
        "        video_data,labels = video_data.to(device), labels.to(device)\n",
        "        #print(video_data.shape)     [32, 8, 3, 224, 224])      B,T,C,H,W\n",
        "                                                                #[BT,HW,C]\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        prediction = model(video_data)\n",
        "\n",
        "        loss = loss_criterion(prediction,labels)\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        corrects = (torch.argmax(prediction,dim=1)==labels).sum()\n",
        "        bacc = corrects/batch_size\n",
        "        del video_data\n",
        "        del labels\n",
        "\n",
        "        gc.collect()\n",
        "        \n",
        "        #tb_writer.add_scalar(\"Train/Loss\",loss.item(),((len(loader))*(epoch-1))+batch_id)\n",
        "        \n",
        "        \n",
        "        print(f\"\\n[Train Epoch]: {epoch} Train Loss: {loss.item()}, Batch Acc is {bacc.item()}\")\n",
        "\n",
        "    return total_epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "elegant-tulsa",
      "metadata": {
        "id": "elegant-tulsa"
      },
      "outputs": [],
      "source": [
        "# validation step for every epoch\n",
        "def val_step(loader,epoch=None):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    corrects=0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_id, (video_data,labels) in enumerate(loader):\n",
        "\n",
        "            video_data,labels = (video_data).to(device), labels.to(device)\n",
        "\n",
        "            prediction = model(video_data)\n",
        "            \n",
        "            loss = loss_criterion(prediction,labels)\n",
        "            total_loss += loss.item()\n",
        "            corrects+= (torch.argmax(prediction,dim=1)==labels).sum()\n",
        "    \n",
        "    accuracy = corrects/(len(loader)*batch_size)\n",
        "    \n",
        "    print(f\"\\n[Val Epoch]: {epoch} , Accuracy: {accuracy}, Valid Loss: {loss.item()}\")\n",
        "    #tb_writer.add_scalar(\"Validation/Loss\",loss.item(),epoch)\n",
        "    #tb_writer.add_scalar(\"Validation/Accuracy\",accuracy,epoch)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "MHl6bllSMGJX",
      "metadata": {
        "id": "MHl6bllSMGJX"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "df7fe42b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder.blocks.0.fc1.weight\n",
            "encoder.blocks.0.fc1.bias\n",
            "encoder.blocks.0.conv.weight\n",
            "encoder.blocks.0.conv.bias\n",
            "encoder.blocks.0.fc2.weight\n",
            "encoder.blocks.0.fc2.bias\n",
            "encoder.blocks.1.fc1.weight\n",
            "encoder.blocks.1.fc1.bias\n",
            "encoder.blocks.1.conv.weight\n",
            "encoder.blocks.1.conv.bias\n",
            "encoder.blocks.1.fc2.weight\n",
            "encoder.blocks.1.fc2.bias\n",
            "encoder.blocks.2.fc1.weight\n",
            "encoder.blocks.2.fc1.bias\n",
            "encoder.blocks.2.conv.weight\n",
            "encoder.blocks.2.conv.bias\n",
            "encoder.blocks.2.fc2.weight\n",
            "encoder.blocks.2.fc2.bias\n",
            "encoder.blocks.3.fc1.weight\n",
            "encoder.blocks.3.fc1.bias\n",
            "encoder.blocks.3.conv.weight\n",
            "encoder.blocks.3.conv.bias\n",
            "encoder.blocks.3.fc2.weight\n",
            "encoder.blocks.3.fc2.bias\n",
            "encoder.blocks.4.fc1.weight\n",
            "encoder.blocks.4.fc1.bias\n",
            "encoder.blocks.4.conv.weight\n",
            "encoder.blocks.4.conv.bias\n",
            "encoder.blocks.4.fc2.weight\n",
            "encoder.blocks.4.fc2.bias\n",
            "encoder.blocks.5.fc1.weight\n",
            "encoder.blocks.5.fc1.bias\n",
            "encoder.blocks.5.conv.weight\n",
            "encoder.blocks.5.conv.bias\n",
            "encoder.blocks.5.fc2.weight\n",
            "encoder.blocks.5.fc2.bias\n",
            "encoder.blocks.6.fc1.weight\n",
            "encoder.blocks.6.fc1.bias\n",
            "encoder.blocks.6.conv.weight\n",
            "encoder.blocks.6.conv.bias\n",
            "encoder.blocks.6.fc2.weight\n",
            "encoder.blocks.6.fc2.bias\n",
            "encoder.blocks.7.fc1.weight\n",
            "encoder.blocks.7.fc1.bias\n",
            "encoder.blocks.7.conv.weight\n",
            "encoder.blocks.7.conv.bias\n",
            "encoder.blocks.7.fc2.weight\n",
            "encoder.blocks.7.fc2.bias\n",
            "encoder.blocks.8.fc1.weight\n",
            "encoder.blocks.8.fc1.bias\n",
            "encoder.blocks.8.conv.weight\n",
            "encoder.blocks.8.conv.bias\n",
            "encoder.blocks.8.fc2.weight\n",
            "encoder.blocks.8.fc2.bias\n",
            "encoder.blocks.9.fc1.weight\n",
            "encoder.blocks.9.fc1.bias\n",
            "encoder.blocks.9.conv.weight\n",
            "encoder.blocks.9.conv.bias\n",
            "encoder.blocks.9.fc2.weight\n",
            "encoder.blocks.9.fc2.bias\n",
            "encoder.blocks.10.fc1.weight\n",
            "encoder.blocks.10.fc1.bias\n",
            "encoder.blocks.10.conv.weight\n",
            "encoder.blocks.10.conv.bias\n",
            "encoder.blocks.10.fc2.weight\n",
            "encoder.blocks.10.fc2.bias\n",
            "encoder.blocks.11.fc1.weight\n",
            "encoder.blocks.11.fc1.bias\n",
            "encoder.blocks.11.conv.weight\n",
            "encoder.blocks.11.conv.bias\n",
            "encoder.blocks.11.fc2.weight\n",
            "encoder.blocks.11.fc2.bias\n",
            "mlp.0.weight\n",
            "mlp.0.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "worst-bedroom",
      "metadata": {
        "id": "worst-bedroom",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76d55885e4ef4250ab0b81d140e3d048",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Train Epoch]: 1 Train Loss: 5.415647029876709, Batch Acc is 0.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 5.702697277069092, Batch Acc is 0.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 4.9019575119018555, Batch Acc is 0.03125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 4.9359869956970215, Batch Acc is 0.0625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 4.732161998748779, Batch Acc is 0.0625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 4.053545951843262, Batch Acc is 0.109375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 4.019776344299316, Batch Acc is 0.125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 3.969655752182007, Batch Acc is 0.171875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 3.4254684448242188, Batch Acc is 0.265625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 3.3211309909820557, Batch Acc is 0.328125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 3.0150067806243896, Batch Acc is 0.328125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 2.6722981929779053, Batch Acc is 0.359375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 2.3366036415100098, Batch Acc is 0.453125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 2.3261570930480957, Batch Acc is 0.46875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.6199413537979126, Batch Acc is 0.625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.6590170860290527, Batch Acc is 0.65625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.8184585571289062, Batch Acc is 0.578125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.4580423831939697, Batch Acc is 0.6875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.3065998554229736, Batch Acc is 0.625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.1320664882659912, Batch Acc is 0.6875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.7616647481918335, Batch Acc is 0.578125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.9863436818122864, Batch Acc is 0.71875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.1587082147598267, Batch Acc is 0.765625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.1405304670333862, Batch Acc is 0.734375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 1.347034215927124, Batch Acc is 0.625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.8742907643318176, Batch Acc is 0.75\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.8580645322799683, Batch Acc is 0.765625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.9493775963783264, Batch Acc is 0.703125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.9832930564880371, Batch Acc is 0.75\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.819113552570343, Batch Acc is 0.765625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.8203600645065308, Batch Acc is 0.765625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.9054233431816101, Batch Acc is 0.75\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6501766443252563, Batch Acc is 0.828125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.8392475843429565, Batch Acc is 0.765625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6543039679527283, Batch Acc is 0.75\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.7076171040534973, Batch Acc is 0.8125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.8622053265571594, Batch Acc is 0.796875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3296138048171997, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6482349634170532, Batch Acc is 0.8125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6393970847129822, Batch Acc is 0.78125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.733687698841095, Batch Acc is 0.75\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6134053468704224, Batch Acc is 0.78125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.42513763904571533, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5807008147239685, Batch Acc is 0.84375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6125763058662415, Batch Acc is 0.8125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6809043884277344, Batch Acc is 0.796875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4309487044811249, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3969777226448059, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.38428765535354614, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3548978269100189, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5295498371124268, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5322991609573364, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5100162625312805, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.27576714754104614, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.49999362230300903, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3624019920825958, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3855191171169281, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.34602588415145874, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5075383186340332, Batch Acc is 0.84375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.47794660925865173, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17650745809078217, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.40199074149131775, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3891177475452423, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3282250463962555, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.38307929039001465, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4039902091026306, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4858160614967346, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3404886722564697, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4251615107059479, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3999052047729492, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.36486896872520447, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.7093311548233032, Batch Acc is 0.828125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4113253057003021, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.42899036407470703, Batch Acc is 0.859375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5599579215049744, Batch Acc is 0.859375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4607486128807068, Batch Acc is 0.84375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.29874420166015625, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.28191542625427246, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.38496360182762146, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.6587173938751221, Batch Acc is 0.828125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5836642980575562, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.29587504267692566, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4709216356277466, Batch Acc is 0.84375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.30648109316825867, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18216846883296967, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3845018744468689, Batch Acc is 0.859375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.42316049337387085, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4036322832107544, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.5144743919372559, Batch Acc is 0.859375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16813838481903076, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.287450909614563, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4593271017074585, Batch Acc is 0.875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2535429000854492, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.240045964717865, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3437791168689728, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.373634934425354, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.24637340009212494, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.29895031452178955, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.4215586185455322, Batch Acc is 0.859375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.249268040060997, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3209233283996582, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2211657464504242, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20915180444717407, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.47786399722099304, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.378336638212204, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11395199596881866, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17329004406929016, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.32347020506858826, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3158850371837616, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3570062816143036, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.31609734892845154, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.22010283172130585, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2049373984336853, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2874554991722107, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3276467025279999, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.219246044754982, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2034216821193695, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3051862120628357, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.31540191173553467, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3407604396343231, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.338554322719574, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2673318684101105, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3308979570865631, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2212347388267517, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.19857369363307953, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.192278191447258, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16650113463401794, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.28052884340286255, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18961189687252045, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1628110110759735, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20543567836284637, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.26324793696403503, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2951631546020508, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14549215137958527, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.230410635471344, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12993699312210083, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21572206914424896, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.38885289430618286, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2360335886478424, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.24043478071689606, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21352551877498627, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2259340137243271, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.240412637591362, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3232300281524658, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.29120221734046936, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1620866358280182, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.22416897118091583, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3605004847049713, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2511165738105774, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2083675116300583, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.19065749645233154, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1806263029575348, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14780756831169128, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2304665744304657, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2364412397146225, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14154021441936493, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10998932272195816, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.25069376826286316, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.191946342587471, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12732261419296265, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1621970236301422, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21740752458572388, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16734550893306732, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18541425466537476, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13331720232963562, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09591510891914368, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.107459656894207, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21628272533416748, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.23093381524085999, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2178536206483841, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20456235110759735, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1876506507396698, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21438848972320557, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17769591510295868, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15206417441368103, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1620153933763504, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14194892346858978, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.169392392039299, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.24038809537887573, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14713330566883087, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15431605279445648, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15289604663848877, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08359768241643906, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20841945707798004, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13514924049377441, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14321334660053253, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1457798033952713, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18647126853466034, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17757044732570648, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1765630692243576, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10458005219697952, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1892586499452591, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14855949580669403, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3021044135093689, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.22178639471530914, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21942470967769623, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14063239097595215, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18061110377311707, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.07630912959575653, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2254214584827423, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1049838587641716, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09197526425123215, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1258448213338852, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1961706131696701, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16295185685157776, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13530570268630981, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13619324564933777, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3216916620731354, Batch Acc is 0.90625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21661221981048584, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20294088125228882, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10658324509859085, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16724169254302979, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10804562270641327, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2885584533214569, Batch Acc is 0.921875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1204119473695755, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.19971662759780884, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15377691388130188, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.3754126727581024, Batch Acc is 0.890625\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12872529029846191, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17245396971702576, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20160922408103943, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21281445026397705, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.06782793253660202, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18395276367664337, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.21143856644630432, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1469024121761322, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08918008953332901, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11780910193920135, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17316576838493347, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14436882734298706, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1609446108341217, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2231840044260025, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18261833488941193, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18714042007923126, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11218713968992233, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1728544533252716, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13122418522834778, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12183520942926407, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08836858719587326, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.06375967711210251, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11407588422298431, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17723584175109863, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.19954784214496613, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10425684601068497, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.22993014752864838, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09817168861627579, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.22021810710430145, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.07827828824520111, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0848531648516655, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08327353745698929, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18676362931728363, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09888955950737, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14612671732902527, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11423777043819427, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.22537851333618164, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13155898451805115, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12406954914331436, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16078807413578033, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.06940264999866486, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11472668498754501, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10320284217596054, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12940481305122375, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12508459389209747, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.147067129611969, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09132921695709229, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1523597687482834, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2860410809516907, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17310316860675812, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2739984691143036, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13482625782489777, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11529570817947388, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13763286173343658, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15391011536121368, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16637413203716278, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20744945108890533, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08535189181566238, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17449527978897095, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11259283870458603, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13601991534233093, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15242138504981995, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11956481635570526, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.062162309885025024, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10243074595928192, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1837325394153595, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0973927453160286, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12088319659233093, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11698351800441742, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12365815788507462, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16217869520187378, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.18673765659332275, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10975659638643265, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1550101935863495, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09333372861146927, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10393505543470383, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1193365678191185, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1076100692152977, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.151662215590477, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11901430785655975, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10782219469547272, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17113180458545685, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20864999294281006, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.080228291451931, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08511053770780563, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.06723804026842117, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16951720416545868, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09321713447570801, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0954454243183136, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1192529946565628, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10455793887376785, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11655727028846741, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08566156774759293, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09867287427186966, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10452061146497726, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.057555388659238815, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11689585447311401, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11419618129730225, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.07790008187294006, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.06261458992958069, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.054738666862249374, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.2090059071779251, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14180517196655273, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0778549388051033, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09559841454029083, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10643336921930313, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09650059044361115, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09862586110830307, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0954144224524498, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09780760854482651, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11533058434724808, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09070590138435364, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11501692980527878, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0908486545085907, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1423579901456833, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09360599517822266, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1372540295124054, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.06804800033569336, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11553572118282318, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.059314146637916565, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13730153441429138, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13778546452522278, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11035080999135971, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.20823322236537933, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1552191525697708, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09825951606035233, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15793953835964203, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11103244870901108, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0874980241060257, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16929472982883453, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14264743030071259, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15210352838039398, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08253899961709976, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1273110806941986, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11707085371017456, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.14062803983688354, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1394798308610916, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.137423574924469, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.12809784710407257, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.13225257396697998, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15101684629917145, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09934423118829727, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1187647134065628, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09805338829755783, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.053047847002744675, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.15713933110237122, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.056949373334646225, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09069190174341202, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.08097854256629944, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09864810854196548, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10250234603881836, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09214405715465546, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11584968119859695, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.1116672083735466, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09659217298030853, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.16321955621242523, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.11396865546703339, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0837593674659729, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09479942917823792, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.09702687710523605, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.10614857077598572, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0994865670800209, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.0943172425031662, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 1 Train Loss: 0.17166179418563843, Batch Acc is 0.765625\n",
            "\n",
            "[Val Epoch]: 1 , Accuracy: 0.9758731722831726, Valid Loss: 0.1210186779499054\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08168251812458038, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04653750732541084, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04666170850396156, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08402997255325317, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1149914413690567, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.14355576038360596, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0796058177947998, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06024198234081268, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1159239262342453, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06609142571687698, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.14207638800144196, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09823718667030334, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07802988588809967, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0669417455792427, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07788921147584915, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1108776107430458, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08126246184110641, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06514876335859299, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05845899507403374, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09207725524902344, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10493136942386627, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10641801357269287, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1567431390285492, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07220537960529327, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07242406904697418, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1109304279088974, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09162605553865433, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05573432520031929, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06848284602165222, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12400151789188385, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06469902396202087, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08916184306144714, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08402932435274124, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04448568820953369, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08097362518310547, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1567668914794922, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10077264159917831, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08071282505989075, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0906970351934433, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12068021297454834, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06081284210085869, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06230977177619934, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08346528559923172, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08924172818660736, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.048415087163448334, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06779269129037857, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12641870975494385, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06162466108798981, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12780557572841644, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13799385726451874, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07586875557899475, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05397281050682068, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0559711717069149, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11692003160715103, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11589793115854263, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12516473233699799, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0685667097568512, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10054648667573929, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05628219619393349, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.14314284920692444, Batch Acc is 0.9375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05544541776180267, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10255595296621323, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08484085649251938, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0937882512807846, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0768967717885971, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07670893520116806, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06833898276090622, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07042383402585983, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.18542417883872986, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09962089359760284, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07201257348060608, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.058575548231601715, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05663205310702324, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10892441868782043, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06337554007768631, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12694968283176422, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1349998414516449, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05173167586326599, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10318034887313843, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08143599331378937, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10860005021095276, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07463105022907257, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08644930273294449, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07236987352371216, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09506672620773315, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08881054818630219, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.076625294983387, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1272423416376114, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06342868506908417, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09631326794624329, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07231802493333817, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07480768114328384, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08909670263528824, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.15460389852523804, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0686061754822731, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12003649026155472, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09257487207651138, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07937168329954147, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0856861025094986, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06688771396875381, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0954844132065773, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09903638064861298, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08866443485021591, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07688039541244507, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09484150260686874, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08088400959968567, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06833241134881973, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08079586178064346, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10401531308889389, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11205384135246277, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08405238389968872, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05488119646906853, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09952083975076675, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06472038477659225, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08104436844587326, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04847758635878563, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07399587333202362, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07351047545671463, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09136029332876205, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10924167931079865, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12153183668851852, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07718069851398468, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.17091147601604462, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1095307320356369, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.16470873355865479, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.055609311908483505, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06812269240617752, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05708090215921402, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12885217368602753, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08886830508708954, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09971253573894501, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07734096795320511, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10174182802438736, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.15194019675254822, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06674440950155258, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09613081067800522, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08161413669586182, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10565491020679474, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12108633667230606, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0977005586028099, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09744559973478317, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07983221858739853, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06465884298086166, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07542449235916138, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09476593881845474, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11281636357307434, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08004119247198105, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09395530074834824, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07268824428319931, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06746437400579453, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10595705360174179, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07312964648008347, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0846584364771843, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04986775666475296, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07119815796613693, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0634119063615799, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06310882419347763, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1405089944601059, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08672894537448883, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07999692857265472, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04627551883459091, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.03798528015613556, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08270556479692459, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11888481676578522, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08167721331119537, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11470139771699905, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05804178863763809, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.20043474435806274, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08043956756591797, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07900683581829071, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05819998309016228, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08106313645839691, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06137927994132042, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0581384003162384, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07912904769182205, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08537247776985168, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08277180045843124, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.051145605742931366, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08364343643188477, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10097845643758774, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08273054659366608, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07596932351589203, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08800061792135239, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08047592639923096, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05362159386277199, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05133048817515373, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07311483472585678, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06715737283229828, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06879062205553055, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08007924258708954, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07995843142271042, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11565610766410828, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08102382719516754, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10017175227403641, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07453111559152603, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09623106569051743, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04824313148856163, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0736103355884552, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08872734755277634, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06366866827011108, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1131766214966774, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06989861279726028, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06813421845436096, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0754629373550415, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06564624607563019, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0945044681429863, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09734904766082764, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.051611628383398056, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1272682100534439, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0575377382338047, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09321719408035278, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06886260211467743, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08487718552350998, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06252192705869675, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10244214534759521, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09113503247499466, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09340685606002808, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11023963242769241, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08574552088975906, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06820408999919891, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06862708926200867, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11677215993404388, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09464459121227264, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1039600521326065, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10965844988822937, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0800861045718193, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0972306951880455, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0721636712551117, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09154598414897919, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06651976704597473, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09746260941028595, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07385782897472382, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0657348558306694, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06591670215129852, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13946233689785004, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09838531166315079, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07446976006031036, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11688198149204254, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08116228878498077, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0741763487458229, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13416948914527893, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05294768884778023, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07553710788488388, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06086738780140877, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06657477468252182, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07006216049194336, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06157597526907921, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08320024609565735, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1173224002122879, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06813166290521622, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13158763945102692, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07817388325929642, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07025345414876938, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07098658382892609, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07582560181617737, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09016317129135132, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0860883891582489, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05984561890363693, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07031505554914474, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07334795594215393, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10377463698387146, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.043525081127882004, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11898820102214813, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10595275461673737, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07206951826810837, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11323675513267517, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08816356211900711, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.047173723578453064, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.059520211070775986, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09674123674631119, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.17284929752349854, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07100299000740051, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10138770937919617, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1911771297454834, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10172012448310852, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09279611706733704, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05858601629734039, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.057296354323625565, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07137665152549744, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10798688977956772, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08632764220237732, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08346834033727646, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11395701766014099, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08698683977127075, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07920603454113007, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11495453119277954, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07843460887670517, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0965879037976265, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06172455847263336, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.1227986291050911, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09765704721212387, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0564647801220417, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06668742001056671, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07622464001178741, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06141844764351845, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07740876823663712, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07151632755994797, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07607080787420273, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10001220554113388, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0632224828004837, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0768759548664093, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09827189892530441, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07237524539232254, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0790434181690216, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09632240980863571, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08455287665128708, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07227248698472977, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08148559182882309, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05175085365772247, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.072882279753685, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.16115082800388336, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07446370273828506, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.050153620541095734, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04370838403701782, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08313121646642685, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07571464031934738, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10331302881240845, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09684757888317108, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10129414498806, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04909558594226837, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13537433743476868, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08405382931232452, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0816054716706276, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07889463007450104, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.11093611270189285, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0470297671854496, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04974917694926262, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08509252965450287, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09875231236219406, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07830832898616791, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07820499688386917, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0864427462220192, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07463260740041733, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.09418739378452301, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10906489938497543, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0942024439573288, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08390310406684875, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06244967132806778, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05811809375882149, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06968250870704651, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06868360191583633, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08849634230136871, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.04850700497627258, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13798461854457855, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08439359813928604, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.084443099796772, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06259823590517044, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10544107854366302, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10004061460494995, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08215738832950592, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13875845074653625, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0777219608426094, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06713902205228806, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08794938027858734, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08427678048610687, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06419786810874939, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06055999919772148, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07006295025348663, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06162399426102638, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0730123445391655, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05206994339823723, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0924428328871727, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06969359517097473, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05597342923283577, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08366215229034424, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07605252414941788, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07246525585651398, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06363721936941147, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.0725383386015892, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10745716094970703, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.10591696947813034, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06154542416334152, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.03867713734507561, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08385825157165527, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06048876792192459, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.06368767470121384, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.05756131559610367, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.08078622817993164, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.12968933582305908, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.068184994161129, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.07448551058769226, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 2 Train Loss: 0.13260863721370697, Batch Acc is 0.75\n",
            "\n",
            "[Val Epoch]: 2 , Accuracy: 0.9848345518112183, Valid Loss: 0.12283515185117722\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07654059678316116, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.060434047132730484, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06465315818786621, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06208484247326851, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07097142189741135, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05857633426785469, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09053671360015869, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.048089392483234406, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06845696270465851, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07275780290365219, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08169251680374146, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09068603068590164, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0656697228550911, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09027621150016785, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0805591344833374, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08659309893846512, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06824006140232086, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.058698367327451706, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06693512946367264, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07802635431289673, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07572547346353531, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07008203864097595, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09702540189027786, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06084951385855675, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08134011924266815, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06797594577074051, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0777047798037529, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05379638075828552, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07071845233440399, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06568047404289246, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05337315797805786, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07832509279251099, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06750646978616714, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08384794741868973, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04364819452166557, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0715206041932106, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06391534954309464, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10129866749048233, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04845280200242996, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07477749139070511, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07772617787122726, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09062544256448746, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08368194848299026, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0676620602607727, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06387946009635925, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0967010036110878, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08753654360771179, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06243959814310074, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06094078719615936, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07729864865541458, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05074097588658333, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04941083490848541, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04483257234096527, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06161263585090637, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.058826521039009094, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0704883486032486, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07265916466712952, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06293381750583649, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07516585290431976, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09064431488513947, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0678868442773819, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10116229951381683, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07547429203987122, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0658697858452797, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08786381036043167, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07924793660640717, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0747501403093338, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06171819567680359, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06199054792523384, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10527631640434265, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.060581617057323456, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08508971333503723, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06777340173721313, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09460394829511642, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09121257811784744, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.053365740925073624, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06995894759893417, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06260349601507187, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08197662979364395, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06350240856409073, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04946613684296608, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10443276166915894, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07365535944700241, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08484460413455963, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05756597965955734, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08346930146217346, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.045818667858839035, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07540412247180939, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09119749814271927, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11190052330493927, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09421700239181519, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06984183192253113, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08607450127601624, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08246587216854095, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.14086298644542694, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06632537394762039, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08173787593841553, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06812203675508499, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06506042927503586, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.048382632434368134, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.047107499092817307, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04863934963941574, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09054946899414062, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06349009275436401, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05818480625748634, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07693237066268921, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07704053819179535, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09507565200328827, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05256761237978935, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11551406234502792, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0506071075797081, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08628705888986588, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07635954767465591, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09281714260578156, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09458623826503754, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0695224478840828, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06074782833456993, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08867248892784119, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09092803299427032, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07814867049455643, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.053119778633117676, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07012315839529037, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09728433191776276, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07010380923748016, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05383926257491112, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05195978283882141, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07138626277446747, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09074520319700241, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0889410674571991, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05499430000782013, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05863535776734352, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06058221682906151, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07296006381511688, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0817597359418869, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0828918069601059, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07113010436296463, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06497560441493988, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06481003016233444, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08809905499219894, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09979043900966644, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09561027586460114, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.045460451394319534, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0748453140258789, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06533303111791611, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08029905706644058, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10201604664325714, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0462544746696949, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.057112500071525574, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07450322061777115, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.1006854847073555, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10729627311229706, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11480779200792313, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07224526256322861, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07457141578197479, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07510408759117126, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.057869382202625275, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0908622145652771, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05407389998435974, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.1042499840259552, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09515409171581268, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08814708888530731, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.056092601269483566, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.061757639050483704, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06806685775518417, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08296377956867218, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07243672013282776, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0447639562189579, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10272408276796341, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.107667937874794, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10268153250217438, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0683659166097641, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08018536865711212, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07843626290559769, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07217754423618317, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06787082552909851, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06851474195718765, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09452097862958908, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0710507407784462, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05608341842889786, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06916722655296326, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07739610224962234, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06374317407608032, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09365152567625046, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.066491037607193, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10820820182561874, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.048688534647226334, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05395788326859474, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06885706633329391, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0783642902970314, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07404503226280212, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09631580859422684, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08958736062049866, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07305888086557388, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06593771278858185, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09121251106262207, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11212026327848434, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06918452680110931, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07621626555919647, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.12137842178344727, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07037000358104706, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0481320284307003, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07305760681629181, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08325295150279999, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0775744691491127, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05798671022057533, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0583522729575634, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.052710726857185364, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08457694202661514, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06903549283742905, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05742652341723442, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09408353269100189, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04984847828745842, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05104253068566322, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09536801278591156, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08830130100250244, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.052155401557683945, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.057680562138557434, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06697133928537369, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0864097997546196, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06861436367034912, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05876665189862251, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11319983005523682, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06607422232627869, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07179354131221771, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09664031118154526, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06968192756175995, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.13880427181720734, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.048551928251981735, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10309148579835892, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09311854839324951, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06827376782894135, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06286466866731644, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06792712956666946, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06872403621673584, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07345524430274963, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08861933648586273, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.059746552258729935, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07826819270849228, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06661977618932724, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08940500020980835, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08861035853624344, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07779809087514877, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06277892738580704, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06882891058921814, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07425396144390106, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0700928270816803, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08053766936063766, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06428844481706619, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07510459423065186, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09557147324085236, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.12643608450889587, Batch Acc is 0.953125\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05700467899441719, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09830696880817413, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.12471423298120499, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04698806256055832, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07644277811050415, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09240638464689255, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09471084922552109, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0847933366894722, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.054290421307086945, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05835927277803421, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08883519470691681, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07179610431194305, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07219742238521576, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08037255704402924, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07775162905454636, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05305977165699005, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0899752601981163, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08827993273735046, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07401002198457718, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0995374247431755, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0944683849811554, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.03913765028119087, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.043056946247816086, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07105354964733124, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.03932170197367668, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08984202146530151, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06056661158800125, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06214014068245888, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09645652770996094, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05716625973582268, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07019951939582825, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.04547913372516632, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09343395382165909, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07431052625179291, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09333506971597672, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07523098587989807, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05434921756386757, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08673778176307678, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07299885898828506, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.16466882824897766, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07892689108848572, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08098899573087692, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06326379626989365, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07690006494522095, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06046849116683006, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11647187173366547, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08919943869113922, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08030882477760315, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10594622045755386, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0678916946053505, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08137167990207672, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05669981986284256, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10085652023553848, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10119524598121643, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.050541214644908905, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.16270340979099274, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08874362707138062, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.038025882095098495, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05783731862902641, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09961627423763275, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09259544312953949, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0831584706902504, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0694473534822464, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08595258742570877, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06602510064840317, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05272351950407028, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.060580458492040634, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06914819777011871, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06174490973353386, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0737624391913414, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08424130082130432, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07861772924661636, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05836718901991844, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08170860260725021, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10056169331073761, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0526060126721859, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07013402134180069, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10069908201694489, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07052299380302429, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07726824283599854, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.1071675717830658, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09067536145448685, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08518149703741074, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.061523403972387314, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08107510954141617, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09406153857707977, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06709472835063934, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07235711067914963, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06466487050056458, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06118161976337433, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06365486979484558, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0879659354686737, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10578899830579758, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06903544068336487, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.051500674337148666, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.051791366189718246, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.13796110451221466, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08349709212779999, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.1298304945230484, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07212383300065994, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08030354231595993, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.061660729348659515, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07613319158554077, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05393529683351517, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08885253220796585, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07466647028923035, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07112856954336166, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06677909940481186, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09695398062467575, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07159844040870667, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07476021349430084, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08450277149677277, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08572262525558472, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11433105170726776, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.05717163532972336, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.08179625123739243, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.047236450016498566, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.11494995653629303, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0738820731639862, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06528282910585403, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.16373465955257416, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06051752716302872, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07674261927604675, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0675213411450386, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06035812199115753, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.06765332818031311, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.10647991299629211, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.0969712883234024, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.07971028238534927, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09748305380344391, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 3 Train Loss: 0.09082759916782379, Batch Acc is 0.78125\n",
            "\n",
            "[Val Epoch]: 3 , Accuracy: 0.986443042755127, Valid Loss: 0.08580274879932404\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.08102233707904816, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07137103378772736, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07077016681432724, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07904064655303955, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.05527585372328758, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06509599089622498, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06589975208044052, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.08567516505718231, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07891420274972916, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.04945754632353783, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.09926147013902664, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.04216357693076134, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07591398805379868, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.09281615912914276, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.08197636902332306, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06812958419322968, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06268589198589325, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07361099869012833, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.04501616582274437, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.07532255351543427, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.05179587006568909, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06259613484144211, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.042971059679985046, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.11125826090574265, Batch Acc is 0.984375\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.04400801658630371, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.049953099340200424, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.0688716471195221, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06978173553943634, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.05767077952623367, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.0775388851761818, Batch Acc is 1.0\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.12105334550142288, Batch Acc is 0.96875\n",
            "\n",
            "[Train Epoch]: 4 Train Loss: 0.06638561189174652, Batch Acc is 1.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Driving train test loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     train_step(train_loader, epoch)\n\u001b[1;32m      4\u001b[0m     val_step(val_loader, epoch)\n\u001b[1;32m      5\u001b[0m     \u001b[39m#scheduler.step()\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(loader, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# model.encoder.eval()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m total_epoch_loss\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, (video_data,labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m     \u001b[39m# video_data,labels = video_data.to(device), labels.to(device)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     video_data,labels \u001b[39m=\u001b[39m video_data\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     \u001b[39m#print(video_data.shape)     [32, 8, 3, 224, 224])      B,T,C,H,W\u001b[39;00m\n\u001b[1;32m     13\u001b[0m                                                             \u001b[39m#[BT,HW,C]\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
            "Cell \u001b[0;32mIn[5], line 73\u001b[0m, in \u001b[0;36mUCFDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m idxs:\n\u001b[1;32m     72\u001b[0m     frame \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(vid[k]\u001b[39m.\u001b[39masnumpy())\n\u001b[0;32m---> 73\u001b[0m     frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(frame)\n\u001b[1;32m     74\u001b[0m     imgs\u001b[39m.\u001b[39mappend(frame)\n\u001b[1;32m     75\u001b[0m imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(imgs)\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
            "File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/PIL/Image.py:2192\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[1;32m   2185\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[1;32m   2186\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2187\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2188\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2189\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2190\u001b[0m         )\n\u001b[0;32m-> 2192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Driving train test loop\n",
        "for epoch in tqdm(range(1,epochs+1)):\n",
        "    train_step(train_loader, epoch)\n",
        "    val_step(val_loader, epoch)\n",
        "    #scheduler.step()\n",
        "    torch.save(model.state_dict(),\"resnet_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddef979a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/z3qian/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/z3qian/miniconda3/envs/dinov2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class '__main__.MLP'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (6): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (7): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (6): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (7): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (8): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (9): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (10): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (11): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (12): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (13): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (14): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (15): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (16): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (17): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (18): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (19): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (20): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (21): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (22): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (23): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (24): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (25): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (26): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (27): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (28): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (29): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (30): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (31): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (32): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (33): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (34): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (35): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=101, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "\n",
        "resnet152 = models.resnet152(pretrained=True)\n",
        "modules=list(resnet152.children())[:-1]\n",
        "resnet152=nn.Sequential(*modules)\n",
        "for p in resnet152.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "model = MLP(2048,512,101,resnet152)\n",
        "a = torch.load('resnet_model.pth')\n",
        "print(type(a))\n",
        "#model.load_state_dict(torch.load('resnet_model.pth'))\n",
        "model = torch.load('resnet_model.pth')\n",
        "model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "r4ptG3LVMKXX",
      "metadata": {
        "id": "r4ptG3LVMKXX"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "GHHJtUrmkjb1",
      "metadata": {
        "id": "GHHJtUrmkjb1"
      },
      "outputs": [],
      "source": [
        "# test the trained model  \n",
        "def test_model(loader):\n",
        "\n",
        "    model.eval()\n",
        "    corrects=0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_id, (input_data,labels) in enumerate(loader):\n",
        "            \n",
        "            input_data = input_data.to(device)\n",
        "            \n",
        "            labels = labels.to(device)\n",
        "            prediction = model(input_data)\n",
        "            loss = loss_criterion(prediction,labels)\n",
        "            total_loss += loss.item()\n",
        "            corrects+= (torch.argmax(prediction,dim=1)==labels).sum()\n",
        "    \n",
        "    accuracy = corrects/(len(loader)*test_batch_size)\n",
        "    print(f\"Test Accuracy: {accuracy}, Test Loss: {total_loss}\")\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "proprietary-shell",
      "metadata": {
        "id": "proprietary-shell"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9969677925109863, Test Loss: 831.5311852202867\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(0.9970, device='cuda:0')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "test_model(test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "BAsvl78-MO_D",
      "metadata": {
        "id": "BAsvl78-MO_D"
      },
      "source": [
        "## Visualize results on tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "increasing-delaware",
      "metadata": {
        "id": "increasing-delaware"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cqqvXqp4G2uI",
      "metadata": {
        "id": "cqqvXqp4G2uI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ViViT_UCF101.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
