{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28216,"status":"ok","timestamp":1685478952614,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"PLzwChbevmfk","outputId":"64ef3453-513f-4217-d302-249577649a01"},"outputs":[],"source":["# Run it once (in one session)\n","# !pip install decord\n","# !pip install einops\n","# !pip install icecream\n","# !pip install rarfile\n","# !pip install unrar"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7995,"status":"ok","timestamp":1685479062270,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"0YxhuZ85swSE"},"outputs":[],"source":["# Imports\n","import torch\n","from torch import nn, einsum\n","from torch.nn import functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision as tv\n","from torch.utils.data import random_split, DataLoader,Dataset\n","import time\n","import random\n","import math\n","import decord\n","import numpy as np\n","import gc\n","from einops import rearrange, repeat,reduce\n","from einops.layers.torch import Rearrange\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from icecream import ic\n","from torchvision.datasets import DatasetFolder\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader, random_split\n","import os\n","import rarfile\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader, random_split\n","import torch.utils.data as data\n","from torchvision import transforms\n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","import shutil\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import cv2\n","import operator\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1685479062271,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"NCvAjgaBtRCS","outputId":"c7437b06-a633-407f-e693-7faec7decc0a"},"outputs":[{"data":{"text/plain":["'cuda:0'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# set device\n","frames_per_clip = 8\n","writer = SummaryWriter()\n","device ='cuda:0' if torch.cuda.is_available() else 'cpu'\n","device"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_3Nv_NnzbWKU"},"source":["# DOWNLOADING DATA FROM SOURCE WEBSITE"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21471,"status":"ok","timestamp":1685478974078,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"k6MgONYtI5Om","outputId":"3dacf360-9988-4be7-9574-3a1789f65d2d"},"outputs":[],"source":["# # once per session/runtime\n","# !wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4LVd8ijibahB"},"source":["# DATA DIRECTORY SETTINGS"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Es-sEYkeJCvB"},"outputs":[],"source":["# # once per session/runtime\n","\n","# rar_path = './hmdb51_org.rar'  \n","# extract_path = './dataset' \n","\n","# with rarfile.RarFile(rar_path, 'r') as rar:\n","#     rar.extractall(extract_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"bhalxfoYLmEo"},"outputs":[],"source":["# # once per session/runtime\n","\n","# direcs = os.listdir(extract_path)\n","\n","# for i in direcs:\n","#   with rarfile.RarFile(f\"dataset/{i}\", 'r') as rar:\n","#     rar.extractall(f\"data/{i.split('.')[0]}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"32FfM0Y8UHgP"},"outputs":[],"source":["# once per session/runtime\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","class Frame:\n","    def __init__(self, id, frame, value):\n","        self.id = id\n","        self.frame = frame\n","        self.value = value\n","\n","    def __lt__(self, other):\n","        if self.id == other.id:\n","            return self.id < other.id\n","        return self.id < other.id\n","\n","    def __gt__(self, other):\n","        return other.__lt__(self)\n","\n","    def __eq__(self, other):\n","        return self.id == other.id and self.id == other.id\n","\n","    def __ne__(self, other):\n","        return not self.__eq__(other)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xEu8eeCWaa1k"},"source":["# DATA LOADING"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"JYSQszRXU8Rw"},"outputs":[],"source":["# Dataset Class\n","class HMDB51Dataset(data.Dataset):\n","    def __init__(self, dataset_dir, frames_per_clip=16):\n","        super().__init__()\n","        self.dataset_dir = dataset_dir\n","        self.frames_per_clip = frames_per_clip\n","        self.video_list = []\n","        self.labels = []\n","\n","        # Get the list of video directories\n","        video_dirs = sorted(os.listdir(dataset_dir))\n","        \n","        for label, video_dir in enumerate(video_dirs):\n","            video_files = os.listdir(os.path.join(dataset_dir, video_dir))\n","            self.video_list.extend([os.path.join(video_dir, video_file) for video_file in video_files])\n","            self.labels.extend([label] * len(video_files))\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, idx):\n","        video_path = os.path.join(self.dataset_dir, self.video_list[idx])\n","        cap = cv2.VideoCapture(str(video_path))\n","\n","\n","        curr_frame = None\n","        prev_frame = None\n","\n","        frame_diffs = []\n","        frames = []\n","        ret, frame = cap.read()\n","        i = 1\n","\n","        while(ret):\n","            luv = cv2.cvtColor(frame, cv2.COLOR_BGR2LUV)\n","            curr_frame = luv\n","            if curr_frame is not None and prev_frame is not None:\n","                #logic here\n","                diff = cv2.absdiff(curr_frame, prev_frame)\n","                count = np.sum(diff)\n","                frame_diffs.append(count)\n","                frame = Frame(i, frame, count)\n","                frames.append(frame)\n","            prev_frame = curr_frame\n","            i = i + 1\n","            ret, frame = cap.read()\n","        \"\"\"\n","            cv2.imshow('frame',luv)\n","            if cv2.waitKey(1) & 0xFF == ord('q'):\n","                break\n","        \"\"\"\n","        cap.release()\n","        # vid = decord.VideoReader(video_path, ctx=decord.cpu(0))\n","        # nframes = len(vid)\n","\n","        # # If the number of frames in the video is less than frames_per_clip, repeat the frames\n","        # if nframes <= self.frames_per_clip:\n","        #     frame_idxs = torch.arange(0, self.frames_per_clip) % nframes\n","        # # Else, sample uniformly separated frames\n","        # else:\n","        #     frame_idxs = torch.linspace(0, nframes - 1, self.frames_per_clip).long()\n","\n","        # frames = []\n","        # for frame_idx in frame_idxs:\n","        #     frame_idx = frame_idx.item()  # Convert to scalar value\n","        #     frame = Image.fromarray(vid[frame_idx].asnumpy())\n","        #     frame = self.transform(frame)\n","        #     frames.append(frame)\n","        frames.sort(key=operator.attrgetter(\"value\"), reverse=True)\n","        keyframes = []\n","        for keyframe in frames[:frames_per_clip]:\n","            frame = Image.fromarray(keyframe.frame)\n","            frame = self.transform(frame)\n","            keyframes.append(frame)\n","        \n","        keyframes = torch.stack(keyframes)\n","\n","        label = self.labels[idx]\n","        #print('Frame for key frame',keyframes.shape)\n","        return keyframes, label"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"AwQ3zMsXU_kn"},"outputs":[],"source":["# Directory of the HMDB51 dataset\n","dataset_dir = \"./data\"\n","\n","# Instantiate the dataset\n","hmdb51_dataset = HMDB51Dataset(dataset_dir, frames_per_clip=frames_per_clip)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JJd7t1xHal8I"},"source":["# TRAIN TEST SPLIT"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"kMkhM0kzVBkf"},"outputs":[],"source":["# Split the dataset into train, validation, and test sets\n","train_len = int(0.7 * len(hmdb51_dataset))\n","test_len = len(hmdb51_dataset) - train_len - (len(hmdb51_dataset) - train_len)//2\n","train_data, val_data,test_data = torch.utils.data.random_split(hmdb51_dataset, [train_len,test_len, len(hmdb51_dataset) - test_len - train_len])\n","\n","# Data loading parameters\n","batch_size = 32\n","test_batch_size = 1\n","num_workers = 0\n","pin_memory = True\n","num_classes = len(set(hmdb51_dataset.labels))\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TOgb3MLjVDb2"},"outputs":[],"source":["# Dataloaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_batch_size, num_workers=num_workers, pin_memory=pin_memory)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1685378429669,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"6uRDrPMCVDyL","outputId":"411f1678-775c-41cc-dc26-24ad0c84d588"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train samples: 4736\n","Validation samples: 1015\n","Test samples: 1015\n"]}],"source":["# Instantiate and create train-val-test split\n","# train_val_data = HMDB51Dataset(dataset_dir, frames_per_clip=frames_per_clip)\n","# train_len = int(0.7 * len(hmdb51_dataset))\n","# test_len = len(hmdb51_dataset) - train_len - (len(hmdb51_dataset) - train_len)//2\n","# train_val_split = [train_len,test_len, len(hmdb51_dataset) - test_len - train_len]\n","# train_data, val_data,test_data = random_split(train_val_data, train_val_split)\n","# test_data = HMDB51Dataset(dataset_dir, frames_per_clip=frames_per_clip)\n","\n","# Print the number of samples in each split\n","print(f\"Train samples: {len(train_data)}\")\n","print(f\"Validation samples: {len(val_data)}\")\n","print(f\"Test samples: {len(test_data)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JgEk-y_vawwB"},"source":["# DEFINING MODEL"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"b4qEV8TIWtPR"},"outputs":[],"source":["class MLP(nn.Module):\n","    \"\"\"\n","    Builds a simple feed forward network\n","    Args:\n","    - dim: (int) - inner dimension of embeddings\n","    - inner_dim: (int) - dimension of transformer head\n","    - n_class: (int) - number of output classes\n","    - encoder: the DinoVisionTransformer encoder\n","    \"\"\"\n","    def __init__(self, dim, inner_dim, n_class, encoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, n_class)\n","        )\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape\n","        x = x.reshape(B*T, C, H, W)\n","        output = self.encoder(x)\n","        output = output.reshape(B, T, -1)\n","        avg = output.mean(dim=1)  # Average pooling over time\n","        return self.mlp(avg)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1988,"status":"ok","timestamp":1685378432273,"user":{"displayName":"Math Fever","userId":"04770446737635186984"},"user_tz":-300},"id":"9KA97YTSWvSe","outputId":"00fdce34-4d66-417c-91c1-1503683f1475"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /home/z3qian/.cache/torch/hub/facebookresearch_dinov2_main\n"]},{"data":{"text/plain":["MLP(\n","  (encoder): DinoVisionTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n","      (norm): Identity()\n","    )\n","    (blocks): ModuleList(\n","      (0-11): 12 x NestedTensorBlock(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): MemEffAttention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): LayerScale()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): LayerScale()\n","        (drop_path2): Identity()\n","        (fc1): Linear(in_features=384, out_features=384, bias=True)\n","        (conv): Conv3d(384, 384, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), groups=384)\n","        (fc2): Linear(in_features=384, out_features=384, bias=True)\n","      )\n","    )\n","    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","    (head): Identity()\n","  )\n","  (mlp): Sequential(\n","    (0): Linear(in_features=384, out_features=51, bias=True)\n","  )\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Instantiate the model\n","dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n","dinov2_vits14.to(device)\n","for param in dinov2_vits14.parameters():\n","    param.requires_grad = False\n","for i in range(12):\n","    #dinov2_vits14.blocks[i].add_module('ln',nn.LayerNorm(384))\n","    dinov2_vits14.blocks[i].add_module('fc1',nn.Linear(384, 384, bias=True))\n","    dinov2_vits14.blocks[i].add_module('conv',nn.Conv3d(384, 384, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), groups=384))\n","    dinov2_vits14.blocks[i].add_module('fc2',nn.Linear(384, 384, bias=True))\n","model = MLP(384, 512, 51, dinov2_vits14)\n","model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3wC4B8W6a3aR"},"source":["## SPECIFYING HYPER-PARAMETERS"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"eON3ZEAhWy3H"},"outputs":[],"source":["# Define the loss function and optimizer\n","lr=0.01\n","epochs = 10\n","decay_rate = 0.95\n","loss_criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=0.9,weight_decay=0.01)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decay_rate)  # stepwise learning rate decay\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F27JEfspa7Fd"},"source":["# MODEL TRAINING & EVALUATION"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Fdwgd-zOW3Z-"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53052163a22142ab83b63893eb6ec592","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","[Train Epoch]: 1 Train Loss: 4.6902079582214355, Batch Acc is 0.0625\n"]},{"ename":"RuntimeError","evalue":"CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m total_epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, (video_data, labels) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader)):\n\u001b[1;32m      7\u001b[0m     video_data, labels \u001b[39m=\u001b[39m video_data\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:680\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m--> 680\u001b[0m     data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39;49mpin_memory\u001b[39m.\u001b[39;49mpin_memory(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pin_memory_device)\n\u001b[1;32m    681\u001b[0m \u001b[39mreturn\u001b[39;00m data\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:70\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m     69\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data])  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[39mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data]\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m     69\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data])  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[39mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data]\n","File \u001b[0;32m~/miniconda3/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:55\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpin_memory\u001b[39m(data, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 55\u001b[0m         \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mpin_memory(device)\n\u001b[1;32m     56\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     57\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["# Training loop\n","val_best = 0\n","for epoch in range(1, epochs + 1):\n","    model.train()\n","    total_epoch_loss = 0\n","    for batch_id, (video_data, labels) in tqdm(enumerate(train_loader)):\n","        video_data, labels = video_data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        prediction = model(video_data)\n","        loss = loss_criterion(prediction, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_epoch_loss += loss.item()\n","        \n","        corrects = (torch.argmax(prediction,dim=1)==labels).sum()\n","        bacc = corrects/batch_size\n","        del video_data\n","        del labels\n","\n","        gc.collect()\n","        print(f\"\\n[Train Epoch]: {epoch} Train Loss: {loss.item()}, Batch Acc is {bacc.item()}\")\n","\n","        # Add any additional training metrics/logging you need\n","    scheduler.step()\n","    # Perform validation at the end of each epoch\n","    model.eval()\n","    total_loss = 0\n","    corrects = 0\n","    with torch.no_grad():\n","        for batch_id, (video_data, labels) in enumerate(val_loader):\n","            video_data, labels = video_data.to(device), labels.to(device)\n","            prediction = model(video_data)\n","            loss = loss_criterion(prediction, labels)\n","            total_loss += loss.item()\n","            corrects += (torch.argmax(prediction, dim=1) == labels).sum()\n","            del video_data\n","            del labels\n","\n","            gc.collect()\n","    accuracy = corrects / (len(val_loader) * batch_size)\n","    writer.add_scalar('Loss', total_loss / len(val_loader), epoch)\n","    writer.add_scalar('Accuracy', accuracy, epoch)\n","    print(f\"\\n[Val Epoch]: {epoch} , Accuracy: {accuracy}, Valid Loss: {total_loss / len(val_loader)}\")\n","    if accuracy > val_best:\n","        torch.save(model,'best_hmdb_model.pth')\n","        val_best = accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KUkL2HwVWQx"},"outputs":[{"data":{"text/plain":["MLP(\n","  (encoder): DinoVisionTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n","      (norm): Identity()\n","    )\n","    (blocks): ModuleList(\n","      (0-11): 12 x NestedTensorBlock(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): MemEffAttention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls1): LayerScale()\n","        (drop_path1): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (ls2): LayerScale()\n","        (drop_path2): Identity()\n","        (fc1): Linear(in_features=384, out_features=384, bias=True)\n","        (conv): Conv3d(384, 384, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), groups=384)\n","        (fc2): Linear(in_features=384, out_features=384, bias=True)\n","      )\n","    )\n","    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","    (head): Identity()\n","  )\n","  (mlp): Sequential(\n","    (0): Linear(in_features=384, out_features=51, bias=True)\n","  )\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Save the trained model\n","#torch.save(model, \"hmdb_st_model.pth\")\n","model = torch.load('best_hmdb_model.pth')\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UazKt6g8WNMn"},"outputs":[],"source":["def test_model(loader):\n","\n","    model.eval()\n","    corrects=0\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch_id, (input_data,labels) in enumerate(loader):\n","            \n","            input_data = input_data.to(device)\n","            \n","            labels = labels.to(device)\n","            prediction = model(input_data)\n","            loss = loss_criterion(prediction,labels)\n","            total_loss += loss.item()\n","            corrects+= (torch.argmax(prediction,dim=1)==labels).sum()\n","    \n","    accuracy = corrects/(len(loader)*test_batch_size)\n","    print(f\"Test Accuracy: {accuracy}, Test Loss: {total_loss}\")\n","\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.6472906470298767, Test Loss: 1564.0274879383696\n"]},{"data":{"text/plain":["tensor(0.6473, device='cuda:0')"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["test_model(test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
